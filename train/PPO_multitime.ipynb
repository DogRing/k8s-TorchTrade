{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def angle_encoding(dt_index,weights=None):\n",
        "    weights = weights or { 'hour':0.1,'day':0.4,'week':0.3,'year':0.2 }\n",
        "    timestamps = dt_index.view('int64') // 10**9\n",
        "    hour_angle = (timestamps%3600) / 3600 * 2 * np.pi\n",
        "    day_angle = (timestamps%86400) / 86400 * 2 * np.pi\n",
        "    week_angle = (timestamps%604800) / 603800 * 2 * np.pi\n",
        "    year_seconds = 365.25 * 86400\n",
        "    year_angle = (timestamps % year_seconds)/year_seconds*2*np.pi\n",
        "    sin_sum = (\n",
        "        weights['hour']*np.sin(hour_angle)+\n",
        "        weights['day']*np.sin(day_angle)+\n",
        "        weights['week']*np.sin(week_angle)+\n",
        "        weights['year']*np.sin(year_angle)\n",
        "    )\n",
        "    cos_sum = (\n",
        "        weights['hour']*np.cos(hour_angle)+\n",
        "        weights['day']*np.cos(day_angle)+\n",
        "        weights['week']*np.cos(week_angle)+\n",
        "        weights['year']*np.cos(year_angle)\n",
        "    )\n",
        "    final_angle = (np.arctan2(sin_sum,cos_sum)+np.pi)/(2*np.pi)\n",
        "    return pd.Series(final_angle,index=dt_index,name='time')\n",
        "\n",
        "class MultiTimeDataset(Dataset):\n",
        "    def __init__(self,path,tick,input_dims,batch_size=64,device='cpu'):\n",
        "        super(MultiTimeDataset,self).__init__()\n",
        "        self.timeframes = input_dims.keys()\n",
        "        file_name = f'{path}{tick}.csv'\n",
        "        df=pd.read_csv(file_name,parse_dates=[0],index_col=[0])\n",
        "        self.x = {k:torch.tensor(\n",
        "            df[[col for col in df.columns if col.startswith(k)]].values,\n",
        "            device=device, dtype=torch.float32\n",
        "        ) for k in self.timeframes}\n",
        "        non_tf_cols = [col for col in df.columns\n",
        "                        if not any(col.startswith(prefix) for prefix in self.timeframes)\n",
        "                        and col != 'close']\n",
        "        self.x['1'] = torch.tensor(\n",
        "            df[non_tf_cols].values,\n",
        "            device=device, dtype=torch.float32\n",
        "        )\n",
        "        self.times = torch.tensor(\n",
        "            angle_encoding(df.index).values,\n",
        "            device=device, dtype=torch.float32\n",
        "        )\n",
        "        time_indices = {\n",
        "            k:np.arange(-int(k)*(v-1),int(k),int(k)) \n",
        "            for k,v in input_dims.items() \n",
        "        }\n",
        "        om = min(arr.min() for arr in time_indices.values())\n",
        "        non_nan_index = df.index.get_loc(df.index[~df.isna().any(axis=1)][0])\n",
        "        self.indices = {\n",
        "            k:arr + abs(om) + non_nan_index\n",
        "            for k,arr in time_indices.items()\n",
        "        }\n",
        "        self.len = len(df)-(abs(om)+non_nan_index)\n",
        "        self.y = df.close.iloc[non_nan_index+abs(om):]\n",
        "        self._precompute_first_batch_indices(batch_size)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return {\n",
        "            tf: (\n",
        "                self.x[tf][self.indices[tf]+idx],\n",
        "                self.times[self.indices[tf]+idx]\n",
        "            )\n",
        "            for tf in self.timeframes\n",
        "        },self.y.iloc[idx]\n",
        "    \n",
        "    def _precompute_first_batch_indices(self,batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        total_samples = len(self)\n",
        "        self.num_batches = (total_samples+ self.batch_size - 1) // self.batch_size\n",
        "        self.first_batch_indices = {}\n",
        "        first_batch_size = min(self.batch_size,total_samples)\n",
        "        for tf in self.timeframes:\n",
        "            tf_indices = self.indices[tf]\n",
        "            indices_matrix = np.empty((first_batch_size,len(tf_indices)),dtype=np.int32)\n",
        "            for i in range(first_batch_size):\n",
        "                indices_matrix[i] = tf_indices + i\n",
        "            self.first_batch_indices[tf] = indices_matrix\n",
        "\n",
        "    def _get_batch_indices(self,batch_idx):\n",
        "        start_idx = batch_idx * self.batch_size\n",
        "        end_idx = min(start_idx + self.batch_size, len(self))\n",
        "        return start_idx, end_idx\n",
        "    \n",
        "    def _prepare_batch(self, batch_idx):\n",
        "        if batch_idx >= self.num_batches:\n",
        "            raise IndexError(\"Batch index out of range\")\n",
        "        start_idx,end_idx = self._get_batch_indices(batch_idx)\n",
        "        current_batch_size = end_idx - start_idx\n",
        "        batch_data = {}\n",
        "        for tf in self.timeframes:\n",
        "            base_indices = self.first_batch_indices[tf][:current_batch_size]\n",
        "            adjusted_indices = base_indices + start_idx\n",
        "            x_batch = self.x[tf][adjusted_indices]\n",
        "            times_batch = self.times[adjusted_indices]\n",
        "            batch_data[tf] = (x_batch,times_batch)\n",
        "        batch_labels = self.y.iloc[start_idx:end_idx]\n",
        "        return batch_data, batch_labels\n",
        "    \n",
        "    def iter_batch(self):\n",
        "        for batch_idx in range(self.num_batches):\n",
        "            yield self._prepare_batch_fast(batch_idx)\n",
        "\n",
        "    def get_batch(self,batch_idx):\n",
        "        return self._prepare_batch(batch_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Krap6rFuVnX"
      },
      "outputs": [],
      "source": [
        "class FourierTimeEmbedding(nn.Module):\n",
        "    \"\"\" 시간 정보를 푸리에 변환을 사용하여 임베딩하는 모듈\n",
        "    시간 정보를 주기적인 특성을 가진 고차원 벡터로 변환합니다.\n",
        "    \n",
        "    Args:\n",
        "        embed_dim (int): 출력 임베딩 차원\n",
        "        num_bands (int): 푸리에 변환에 사용할 주파수 밴드 수\n",
        "    \"\"\"\n",
        "    def __init__(self,embed_dim=128,num_bands=32):\n",
        "        super().__init__()\n",
        "        self.num_bands = num_bands\n",
        "        self.embed_dim = embed_dim\n",
        "        self.fc1 = nn.Linear(2*num_bands,embed_dim)\n",
        "        self.fc2 = nn.Linear(embed_dim,embed_dim)\n",
        "        self.activation = nn.GELU()\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self,t):\n",
        "        # t: [batch,seq,1]\n",
        "        coeffs = torch.linspace(0,1,self.num_bands,device=t.device)\n",
        "        embed = torch.einsum('bs,n->bsn',t,coeffs) * 2 * torch.pi\n",
        "        embed = torch.cat([torch.sin(embed),torch.cos(embed)],dim=-1)\n",
        "        embed = self.fc1(embed)\n",
        "        embed = self.activation(embed)\n",
        "        embed = self.fc2(embed)\n",
        "        return self.norm(embed)\n",
        "\n",
        "class ExecutionHyridModule(nn.Module):\n",
        "    \"\"\" 1분봉 데이터를 처리하는 CNN-GRU 하이브리드 모듈\n",
        "    CNN으로 지역적 특징을 추출하고 GRU로 시계열 정보를 처리합니다.\n",
        "    \n",
        "    Args:\n",
        "        input_dim (int): 입력 특징 차원\n",
        "        time_dim (int): 시간 임베딩 차원\n",
        "        hidden_size (int): 은닉층 크기\n",
        "    \"\"\"\n",
        "    def __init__(self,input_dim,time_dim=128,hidden_size=64):\n",
        "        super().__init__()\n",
        "        self.time_embed = FourierTimeEmbedding(time_dim)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(input_dim+time_dim,hidden_size,5,padding=2),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.Conv1d(hidden_size,hidden_size,3,padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(hidden_size)\n",
        "        )\n",
        "        self.gru = nn.GRU(hidden_size,hidden_size,batch_first=True,\n",
        "                          bidirectional=True,num_layers=2,dropout=0.3)\n",
        "        self.proj = nn.Linear(hidden_size*2, 128)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.norm = nn.LayerNorm(128)\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        # x: [B,T,F], t: [B,T,1]\n",
        "        t_emb = self.time_embed(t)\n",
        "        x = torch.cat([x,t_emb],dim=-1).permute(0,2,1)\n",
        "        conv_out = self.conv(x).permute(0,2,1)\n",
        "        gru_out,_ = self.gru(conv_out)\n",
        "        return self.norm(self.dropout(self.proj(gru_out)))\n",
        "\n",
        "class MultiScaleLSTM(nn.Module):\n",
        "    \"\"\" 15분/4시간 봉 데이터를 위한 멀티스케일 LSTM 모듈\n",
        "    여러 시간 스케일에서 LSTM을 적용하여 다양한 시간대의 패턴을 포착합니다.\n",
        "    \n",
        "    Args:\n",
        "        input_dim (int): 입력 특징 차원\n",
        "        time_dim (int): 시간 임베딩 차원\n",
        "        scales (list): 각 LSTM이 처리할 시간 스케일 목록\n",
        "    \"\"\"\n",
        "    def __init__(self,input_dim,time_dim=128,scales=[5,10,20]):\n",
        "        super().__init__()\n",
        "        self.time_embed = FourierTimeEmbedding(time_dim)\n",
        "        self.lstms = nn.ModuleList([\n",
        "            nn.LSTM(\n",
        "                input_size=input_dim+time_dim,\n",
        "                hidden_size=64,\n",
        "                num_layers=2,\n",
        "                dropout=0.3,\n",
        "                batch_first=True)\n",
        "            for _ in scales\n",
        "        ])\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in scales])\n",
        "        self.attn = nn.MultiheadAttention(64*len(scales),8,batch_first=True)\n",
        "        self.proj = nn.Linear(64*len(scales),128)\n",
        "        self.norm = nn.LayerNorm(128)\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        t_emb = self.time_embed(t)\n",
        "        x_in = torch.cat([x,t_emb],dim=-1)\n",
        "        outputs = []\n",
        "        for i,lstm in enumerate(self.lstms):\n",
        "            out,_ = lstm(x_in)\n",
        "            out = self.dropouts[i](out)\n",
        "            outputs.append(out)\n",
        "        concat = torch.cat(outputs,dim=-1)\n",
        "        attn_out,_ = self.attn(concat,concat,concat)\n",
        "        return self.norm(self.proj(attn_out))\n",
        "\n",
        "class HierarchicalTransformer(nn.Module):\n",
        "    \"\"\" 1시간/일 봉 데이터를 처리하는 트랜스포머 기반 모듈\n",
        "    자기 주의 메커니즘을 통해 장기 의존성을 포착합니다.\n",
        "    \n",
        "    Args:\n",
        "        input_dim (int): 입력 특징 차원\n",
        "        time_dim (int): 시간 임베딩 차원\n",
        "        nhead (int): 멀티헤드 어텐션의 헤드 수\n",
        "        num_layers (int): 트랜스포머 인코더 층 수\n",
        "    \"\"\"\n",
        "    def __init__(self,input_dim,time_dim=128,nhead=8,num_layers=4):\n",
        "        super().__init__()\n",
        "        self.time_embed = FourierTimeEmbedding(time_dim)\n",
        "        self.input_proj = nn.Linear(input_dim+time_dim,128)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=128,nhead=nhead,dim_feedforward=512,\n",
        "            dropout=0.2,activation='gelu',batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer,num_layers)\n",
        "        self.norm = nn.LayerNorm(128)\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        t_emb = self.time_embed(t)\n",
        "        x_in = torch.cat([x,t_emb],dim=-1)\n",
        "        projected = self.input_proj(x_in)\n",
        "        return self.norm(self.encoder(projected))\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\" 다중 시간대 특징을 통합하는 교차 모달 어텐션 모듈\n",
        "    서로 다른 시간대의 특징들 간의 관계를 학습합니다.\n",
        "    \n",
        "    Args:\n",
        "        num_timeframes (int): 처리할 시간대 수\n",
        "        embed_dim (int): 특징 임베딩 차원\n",
        "        heads (int): 어텐션 헤드 수\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dims, embed_dim=128, heads=8):\n",
        "        super().__init__()\n",
        "        self.timeframes = input_dims.keys()\n",
        "        num_timeframes = len(self.timeframes)\n",
        "        self.projections = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(dim,embed_dim),\n",
        "                nn.GELU(),\n",
        "                nn.LayerNorm(embed_dim)\n",
        "            )\n",
        "            for tf, dim in input_dims.items()\n",
        "        ])\n",
        "        self.attentions = nn.ModuleList([\n",
        "            nn.MultiheadAttention(embed_dim, heads, dropout=0.2, batch_first=True)\n",
        "            for _ in range(num_timeframes)\n",
        "        ])\n",
        "        self.norms = nn.ModuleList([\n",
        "            nn.LayerNorm(embed_dim)\n",
        "            for _ in range(num_timeframes)\n",
        "        ])\n",
        "        self.timeframe_weights = nn.Parameter(torch.ones(num_timeframes)/num_timeframes)\n",
        "        self.final_norm = nn.LayerNorm(embed_dim * num_timeframes)\n",
        "        self.final_dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, features):\n",
        "        features = [proj(feat.transpose(1,2)).transpose(1,2)\n",
        "            for proj,feat in zip(self.projections,features)]\n",
        "        # [B, K, T, D] 형태로 변환\n",
        "        context = torch.stack(features, dim=1)\n",
        "        B, K, T, D = context.shape\n",
        "        # 배치와 시퀀스 차원 결합\n",
        "        context = context.view(B*T, K, D)\n",
        "        # 각 시간대별 어텐션 적용\n",
        "        attn_outs = []\n",
        "        for i, (attn, norm) in enumerate(zip(self.attentions, self.norms)):\n",
        "            # 현재 시간대를 쿼리로 사용\n",
        "            query = context[:, i:i+1]\n",
        "            # 어텐션 및 잔차 연결\n",
        "            out, _ = attn(query, context, context)\n",
        "            attn_outs.append(norm(out + query))\n",
        "        # 모든 시간대의 특징 결합\n",
        "        weights = F.softmax(self.timeframe_weights,dim=0)\n",
        "        combined = torch.stack(attn_outs,dim=1)*weights.view(1,K,1,1)\n",
        "        fused = combined.view(B,T,K*D)\n",
        "        return self.final_norm(self.final_dropout(fused))\n",
        "\n",
        "class EnhancedMultiTimeframeModel(nn.Module):\n",
        "    \"\"\" 다중 시간대 데이터를 처리하는 강화학습 모델\n",
        "    각 시간대별로 특화된 모듈을 사용하여 특징을 추출하고,\n",
        "    이를 통합하여 행동(actor)과 가치(critic) 예측을 수행합니다.\n",
        "    \n",
        "    Args:\n",
        "        feature_dims (dict): 각 시간대별 입력 특징 차원을 담은 딕셔너리\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dims, input_dims, action_dim):\n",
        "        super().__init__()\n",
        "        # 입력된 시간대 저장\n",
        "        self.timeframes = list(feature_dims.keys())\n",
        "        \n",
        "        # 각 시간대별 특화된 모듈 초기화\n",
        "        self.modules_dict = nn.ModuleDict()\n",
        "        for tf, dim in feature_dims.items():\n",
        "            # 시간대에 따라 적절한 모듈 선택\n",
        "            if int(tf) <= 20:  # 1분봉\n",
        "                self.modules_dict[tf] = ExecutionHyridModule(dim)\n",
        "            elif int(tf) <= 100:  # 1시간 이하\n",
        "                self.modules_dict[tf] = MultiScaleLSTM(dim)\n",
        "            else:  # 1시간 초과\n",
        "                self.modules_dict[tf] = HierarchicalTransformer(dim)\n",
        "        \n",
        "        # 특징 융합을 위한 어텐션 모듈\n",
        "        self.fusion = CrossModalAttention(\n",
        "            input_dims,\n",
        "            embed_dim=128  # 각 모듈의 출력 차원\n",
        "        )\n",
        "        \n",
        "        # 행동과 가치 예측을 위한 헤드\n",
        "        fusion_dim = 128 * len(self.timeframes)  # 융합된 특징의 차원\n",
        "    \n",
        "        # 행동 (롱/숏/중립)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512,256),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, action_dim)  # 3개의 행동: [롱, 중립, 숏]\n",
        "        )\n",
        "        \n",
        "        # Critic 네트워크 (가치 예측)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512,256),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        features = []\n",
        "        # 입력된 모든 시간대에 대해 처리\n",
        "        for tf in self.timeframes:\n",
        "            data, time = inputs[tf]\n",
        "            out = self.modules_dict[tf](data, time)\n",
        "            features.append(out)\n",
        "        # 특징 융합\n",
        "        fused = self.fusion(features)\n",
        "        logits = self.actor(fused)\n",
        "        dist = Categorical(logits=logits)\n",
        "        value = self.critic(fused)\n",
        "        return dist, value\n",
        "    def get_action(self,inputs,deterministic=False,mode='last'):\n",
        "        dist,value = self.forward(inputs)\n",
        "        if deterministic:\n",
        "            action = torch.argmax(dist.probs,dim=-1)\n",
        "        else:\n",
        "            action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        if mode == 'last':\n",
        "            return action[:,-1],log_prob[:,-1],entropy[:,-1],value[:,-1]\n",
        "        elif mode == 'mean':\n",
        "            return action[:,-1],log_prob.mean(dim=1),entropy.mean(dim=1),value.mean(dim=1,keepdim=True)\n",
        "        else:\n",
        "            return action,log_prob,entropy,value\n",
        "    \n",
        "    def get_logprob(self,inputs,action,mode='last'):\n",
        "        dist,value = self.forward(inputs)\n",
        "        B,T,_ = dist.probs.shape\n",
        "        action = action.unsqueeze(1).expand(B, T)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        if mode == 'last':\n",
        "            return log_prob[:, -1], entropy[:, -1], value[:, -1]\n",
        "        elif mode == 'mean':\n",
        "            return log_prob.mean(dim=1), entropy.mean(dim=1), value.mean(dim=1, keepdim=True)\n",
        "        else:\n",
        "            return log_prob,entropy,value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GAMMA = 0.99\n",
        "GAE_LAMBDA = 0.95\n",
        "CLIP_EPSILON = 0.2\n",
        "CRITIC_DISCOUNT = 0.5\n",
        "ENTROPY_BETA = 0.01\n",
        "LEARNING_RATE = 3e-4\n",
        "PPO_EPOCHS = 30\n",
        "MINI_BATCH_SIZE = 64\n",
        "MAX_GRAD_NORM = 0.5\n",
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self):\n",
        "        self.states_index = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "    def push(self,state_index,action,reward,value,log_prob,done):\n",
        "        self.states_index.append(state_index)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.values.append(value)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "    def get(self):\n",
        "        return (\n",
        "            self.states_index,\n",
        "            torch.stack(self.actions),\n",
        "            torch.stack(self.rewards),\n",
        "            torch.stack(self.values),\n",
        "            torch.stack(self.log_probs),\n",
        "            torch.tensor(self.dones,device=self.actions[0].device)\n",
        "        )\n",
        "    def clear(self):\n",
        "        self.states_index.clear()\n",
        "        self.actions.clear()\n",
        "        self.rewards.clear()\n",
        "        self.values.clear()\n",
        "        self.log_probs.clear()\n",
        "        self.dones.clear()\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, feature_dims, input_dims, n_actions, device):\n",
        "        self.policy = EnhancedMultiTimeframeModel(feature_dims, input_dims, n_actions).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)\n",
        "        self.memory = PPOMemory()\n",
        "        self.mode = \"last\"\n",
        "        self.device = device\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "    def compute_gae(self, next_value, rewards, values, dones):\n",
        "        values = torch.cat([values, next_value.T]).to(torch.float32)\n",
        "        gae = 0\n",
        "        returns = torch.zeros_like(rewards,device=self.device,dtype=torch.float32)\n",
        "        advantages = torch.zeros_like(rewards,device=self.device,dtype=torch.float32)\n",
        "        for steps in reversed(range(len(rewards))):\n",
        "            delta = rewards[steps] + GAMMA * values[steps + 1] * (1 - dones[steps]) - values[steps]\n",
        "            gae = delta + GAMMA * GAE_LAMBDA * (1 - dones[steps]) * gae\n",
        "            advantages[steps] = gae\n",
        "            rewards[steps] = gae + values[steps]\n",
        "        return returns, advantages\n",
        "    def update(self,next_value,dataset):\n",
        "        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'\n",
        "        states_indexs, actions, rewards, values, old_log_probs, dones = self.memory.get()\n",
        "        returns, advantages = self.compute_gae(next_value,rewards,values,dones)\n",
        "        returns = returns.detach()\n",
        "        advantages = advantages.detach()\n",
        "        old_log_probs = [log_prob.detach() for log_prob in old_log_probs]\n",
        "        returns = returns.view(-1,MINI_BATCH_SIZE)\n",
        "        advantages = advantages.view(-1,MINI_BATCH_SIZE)\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "        for _ in range(PPO_EPOCHS):\n",
        "            for batch_idx in states_indexs:\n",
        "                with torch.amp.autocast(device_type=device_type,dtype=torch.float32):\n",
        "                    mb_states,_ = dataset.get_batch(batch_idx)\n",
        "                    mb_actions = actions[batch_idx]\n",
        "                    mb_returns = returns[batch_idx].view(-1)\n",
        "                    mb_advantages = advantages[batch_idx]\n",
        "                    mb_old_log_prob = old_log_probs[batch_idx]\n",
        "                    \n",
        "                    new_log_prob,entropy,values = self.policy.get_logprob(mb_states, mb_actions, self.mode)\n",
        "                    ratio = torch.exp(new_log_prob - mb_old_log_prob)\n",
        "                    surr1 = ratio * mb_advantages\n",
        "                    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON) * mb_advantages\n",
        "                    actor_loss = -torch.min(surr1,surr2).mean()\n",
        "                    critic_loss = F.mse_loss(values.squeeze(-1), mb_returns, reduction='mean')\n",
        "                    loss = actor_loss + CRITIC_DISCOUNT * critic_loss - ENTROPY_BETA * entropy.mean()\n",
        "                \n",
        "                self.optimizer.zero_grad()\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                nn.utils.clip_grad_norm_(self.policy.parameters(),MAX_GRAD_NORM)\n",
        "\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "        self.memory.clear()\n",
        "    def save(self,path):\n",
        "        torch.save(self.policy.state_dict(),path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TradingEnv:\n",
        "    def __init__(self,path,ticks,input_dims,batch_size,max_step,device='cpu'):\n",
        "        self.max_step = max_step\n",
        "        self.datasets = [MultiTimeDataset(path,tick,input_dims,batch_size,device) for tick in ticks]\n",
        "        self.reset(0)\n",
        "    def reset(self,batch_index):\n",
        "        self.dataset = self.datasets[random.randint(0,len(self.datasets)-1)]\n",
        "        self.position = False\n",
        "        self.current_price = None\n",
        "        self.transaction_cost = 0.0005\n",
        "        self.n_step = 0\n",
        "        self.total_reward = 1\n",
        "        self.last_reward = 0\n",
        "        return self.dataset.get_batch(batch_index)\n",
        "    def step(self,actions,labels,batch_index):\n",
        "        if not self.current_price:\n",
        "            self.current_price = labels[0]\n",
        "        actions[0] = 1 if actions[0] == position + 1 else actions[0]\n",
        "\n",
        "        mask = actions != 1\n",
        "\n",
        "        if mask.any():\n",
        "            non_zero_idxs = self.indices[mask]\n",
        "            if len(non_zero_idxs) > 1:\n",
        "                diff = torch.diff(actions[mask])\n",
        "                change_mask = diff != 0\n",
        "                if change_mask.any():\n",
        "                    changes = torch.nonzero(change_mask).squeeze(-1)\n",
        "                    final_idxs = torch.cat([non_zero_idxs[[0]], non_zero_idxs[changes + 1]])\n",
        "                else:\n",
        "                    final_idxs = non_zero_idxs[[0]]\n",
        "            else:\n",
        "                final_idxs = non_zero_idxs\n",
        "        else:\n",
        "            final_idxs = torch.tensor([], device=self.device, dtype=torch.long)\n",
        "\n",
        "        current_prices = torch.zeros_like(labels,device=self.device)\n",
        "        current_prices[final_idxs] = labels[final_idxs]\n",
        "\n",
        "        cum_max_indices = torch.cummax(\n",
        "            torch.where(current_prices != 0,\n",
        "                        self.indices, torch.tensor(-1, device=self.device)\n",
        "            ), dim=0)[0]\n",
        "        current_prices = torch.where(cum_max_indices >= 0, current_prices[cum_max_indices], self.current_price)\n",
        "        current_price = current_prices[-1]\n",
        "        current_prices = torch.roll(current_prices, shifts=1)\n",
        "        current_prices[0] = current_prices[1]\n",
        "\n",
        "        positions = self.position * (-1) ** (self.indices.unsqueeze(1) > final_idxs.unsqueeze(0)).sum(dim=1)\n",
        "        self.position = positions[-1] if final_idxs == n else positions[-1]*-1\n",
        "\n",
        "        profit = (labels - current_prices) / labels * positions\n",
        "        profit[final_idxs] = torch.where(positions[final_idxs] == -1, profit[final_idxs] - self.transaction_cost, profit[final_idxs])\n",
        "\n",
        "        rewards = torch.where(torch.isin(self.indices,final_idxs),profit,profit*0.01)\n",
        "\n",
        "        self.n_step += 1\n",
        "        self.total_reward *= torch.prod(rewards + 1).item()\n",
        "        if (self.total_reward < 0.8) or (self.n_step >= self.max_step):\n",
        "            done = 1\n",
        "        rewards[0] += self.last_reward\n",
        "        rewards = torch.cumsum(rewards,dim=0)\n",
        "        self.last_reward = rewards[-1].item()\n",
        "        states, labels = self.dataset.get_batch(batch_index)\n",
        "        return states, labels, rewards, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"/content/drive/My Drive/ttrade/data/250325/\"\n",
        "ticks = [\"KRW-BTC\",\"KRW-ETC\",\"KRW-DOGE\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "feature_dims = {\n",
        "    '1': 4, '15':6, '60':3, '240':6, '1440':8\n",
        "}\n",
        "\n",
        "input_dims = {\n",
        "    '1':140,'15':15,'60':15,'240':15,'1440':10\n",
        "}\n",
        "param_path = '/content/drive/My Drive/ttrade/pth/ppo-MT1.pth'\n",
        "\n",
        "max_steps = 2048\n",
        "env = TradingEnv(path,ticks,input_dims,MINI_BATCH_SIZE,max_steps,device)\n",
        "agent = PPOAgent(feature_dims, input_dims, 3, device)\n",
        "if os.path.exists(param_path) == True:\n",
        "    agent.policy.load_state_dict(torch.load(param_path,weights_only=True,map_location=device))\n",
        "\n",
        "num_episodes = 30\n",
        "update_interval = 256\n",
        "for episode in range(num_episodes):\n",
        "    batch_index = 0\n",
        "    state, label = env.reset(batch_index)\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for step in range(batch_index, batch_index+max_steps):\n",
        "        with torch.no_grad():\n",
        "            action,log_prob,_,value = agent.policy.get_action(state,deterministic=(random.random() < 0.4))\n",
        "        next_batch_index = batch_index + 1\n",
        "        next_state,next_label,reward,done = env.step(action,label,next_batch_index)\n",
        "        agent.memory.push(batch_index, action, reward, log_prob, value, done)\n",
        "        state = next_state\n",
        "        label = next_label\n",
        "        episode_reward += reward\n",
        "        \n",
        "        if (step + 1) % update_interval == 0 or done:\n",
        "            with torch.amp.autocast(device_type=device.type):\n",
        "                _,_,_,next_value = agent.policy.get_action(\n",
        "                    state, deterministic=True) if not done else (\n",
        "                    None,None,None,torch.zeros_like(value)\n",
        "                )\n",
        "            agent.update(next_value, env.dataset)\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    agent.save(param_path)\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Episode: {episode+1}, Reward: {episode_reward.float().mean()}, batch: {step}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
